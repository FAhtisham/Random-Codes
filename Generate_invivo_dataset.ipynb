{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process DeepSea datset\n",
    "\n",
    "In this notebook, the DeepSea dataset is acquired and parsed to generate a smaller transcription factor dataset, consisting of CTCF, GABP, SP1, SRF, and YY1, for K562 and HepG2 celltypes. The dataset is first downloaded directly from DeepSea webserver and then custom scripts convert these into a h5py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, h5py, scipy.io\n",
    "import numpy as np\n",
    "import subprocess as sp\n",
    "import  tarfile, wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# download DeepSea dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# download deepsea dataset into data folder, if it does not exist\n",
    "if not os.path.isdir('/data/deepsea_train'):\n",
    "    print('downloading DeepSea dataset')\n",
    "    url = 'http://deepsea.princeton.edu/media/code/deepsea_train_bundle.v0.9.tar.gz'\n",
    "    filename = wget.download(url)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = './data/deepsea_train/deepsea_train_bundle.v0.9.tar.gz'\n",
    "if fname.endswith(\"tar.gz\"):\n",
    "    tar = tarfile.open(fname, \"r:gz\")\n",
    "    tar.extractall()\n",
    "    tar.close()\n",
    "elif fname.endswith(\"tar\"):\n",
    "    tar = tarfile.open(fname, \"r:\")\n",
    "    tar.extractall()\n",
    "    tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "my_tar = tarfile.open(fname)\n",
    "my_tar.extractall() # specify which folder to extract to\n",
    "my_tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_DeepSea_subset(filepath, class_range=range(918)):\n",
    "    \"\"\" function to load DeepSea's dataset of specific transcription factors specified \n",
    "        by class_range. The output is a h5py file with the sequences represented\n",
    "        as a 4D tensor for input into Lasagne/Theano convolution layers.  The labels\n",
    "        is a 2D matrix where each row corresponds to a new sequence. \"\"\"\n",
    "    \n",
    "    def data_subset(y, class_range):\n",
    "        \" gets a subset of data in the class_range\"\n",
    "        data_index = []\n",
    "        for i in class_range:\n",
    "            index = np.where(y[:, i] == 1)[0]\n",
    "            data_index = np.concatenate((data_index, index), axis=0)\n",
    "        unique_index = np.unique(data_index)\n",
    "        return unique_index.astype(int)\n",
    "\n",
    "    print(\"loading training data\")\n",
    "    trainmat = h5py.File(os.path.join(filepath,'train.mat'), 'r')\n",
    "    y_train = np.transpose(trainmat['traindata'], axes=(1,0))\n",
    "    index = data_subset(y_train, class_range)\n",
    "    y_train = y_train[:,class_range]\n",
    "    y_train = y_train[index,:]\n",
    "    X_train = np.transpose(trainmat['trainxdata'], axes=(2,1,0)) \n",
    "    X_train = X_train[index,:,:]\n",
    "    X_train = X_train[:,[0,2,1,3],:]\n",
    "    X_train = np.expand_dims(X_train, axis=3)\n",
    "    train = (X_train.astype(np.int8), y_train.astype(np.int8))\n",
    "    \n",
    "    print(\"loading validation data\")\n",
    "    validmat = scipy.io.loadmat(os.path.join(filepath,'valid.mat'))\n",
    "    y_valid = np.array(validmat['validdata'])\n",
    "    index = data_subset(y_valid,class_range)\n",
    "    y_valid = y_valid[:, class_range]\n",
    "    y_valid = y_valid[index,:]\n",
    "    X_valid = np.transpose(validmat['validxdata'], axes=(0,1,2))  \n",
    "    X_valid = X_valid[index,:,:]\n",
    "    X_valid = X_valid[:,[0,2,1,3],:]\n",
    "    X_valid = np.expand_dims(X_valid, axis=3)\n",
    "    valid = (X_valid.astype(np.int8), y_valid.astype(np.int8))\n",
    "    \n",
    "    print(\"loading test data\")\n",
    "    testmat = scipy.io.loadmat(os.path.join(filepath,'test.mat'))\n",
    "    y_test = np.array(testmat['testdata'])\n",
    "    index = data_subset(y_test,class_range)\n",
    "    y_test = y_test[:, class_range]\n",
    "    y_test = y_test[index,:]\n",
    "    X_test = np.transpose(testmat['testxdata'], axes=(0,1,2)) \n",
    "    X_test = X_test[index,:,:]\n",
    "    X_test = X_test[:,[0,2,1,3],:]\n",
    "    X_test = np.expand_dims(X_test, axis=3)\n",
    "    test = (X_test.astype(np.int8), y_test.astype(np.int8))\n",
    "\n",
    "    return train, valid, test \n",
    "\n",
    "def process_DeepSea_subset(train, valid, valid_percentage=0.1):\n",
    "    \"\"\"merge training and validation data, shuffle, and reallocate \n",
    "       based on 90% training and 10% cross-validation \"\"\"\n",
    "    \n",
    "    X_train = np.vstack([train[0], valid[0]])\n",
    "    Y_train = np.vstack([train[1], valid[1]])\n",
    "    index = np.random.permutation(X_train.shape[0])\n",
    "    cutoff = np.round(X_train.shape[0]*valid_percentage).astype(int)\n",
    "    valid = (X_train[:cutoff], Y_train[:cutoff])\n",
    "    train = (X_train[cutoff:], Y_train[cutoff:])\n",
    "    \n",
    "    return train, valid\n",
    "\n",
    "\n",
    "def save_DeepSea_subset(grp, train, valid, test):\n",
    "    \"\"\" save to h5py dataset \"\"\"\n",
    "    print(\"saving datset\")\n",
    "    X_train = grp.create_dataset('X_train', data=train[0], dtype='int8', compression=\"gzip\")\n",
    "    Y_train = grp.create_dataset('Y_train', data=train[1], dtype='int8', compression=\"gzip\")\n",
    "    X_valid = grp.create_dataset('X_valid', data=valid[0], dtype='int8', compression=\"gzip\")\n",
    "    Y_valid = grp.create_dataset('Y_valid', data=valid[1], dtype='int8', compression=\"gzip\")\n",
    "    X_test = grp.create_dataset('X_test', data=test[0], dtype='int8', compression=\"gzip\")\n",
    "    Y_test = grp.create_dataset('Y_test', data=test[1], dtype='int8', compression=\"gzip\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parse subset of DeepSea dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-92-691b4934c597>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading training data\n",
      "loading validation data\n",
      "loading test data\n"
     ]
    }
   ],
   "source": [
    "core_names = ['Arid3a', 'CEBPB', 'FOSL1', 'Gabpa', 'MAFK', 'MAX', \n",
    "              'MEF2A', 'NFYB', 'SP1', 'SRF', 'STAT1', 'YY1']\n",
    "core_index = [592, 602, 344, 345, 635, 636, 349, 642, 359, 361, 661, 369]\n",
    "#core_index =  [547, 602, 344, 345, 635, 636, 218, 642, 237, 238, 535, 369]\n",
    "\n",
    "# save datasets in a hdf5 file under groups HepG2 and K562\n",
    "data_path = './deepsea_train/'\n",
    "\n",
    "# load deep sea dataset\n",
    "train, valid, test = load_DeepSea_subset(data_path, class_range=core_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[592, 602, 344, 345, 635, 636, 349, 642, 359, 361, 661, 369]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(269994, 4, 1000, 1)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training samples for each class\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([27652, 85354, 19724, 34194, 43528, 87290,  9792, 22758, 17450,\n",
       "        7528,  4516, 31146])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"number of training samples for each class\")\n",
    "np.sum(train[1], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train, valid = process_DeepSea_subset(train, valid, valid_percentage=0.1)        \n",
    "with h5py.File('./final_data/invivo_dataset.h5', 'w') as fout:\n",
    "    X_train = fout.create_dataset('X_train', data=train[0], dtype='int8', compression=\"gzip\")\n",
    "    Y_train = fout.create_dataset('Y_train', data=train[1], dtype='int8', compression=\"gzip\")\n",
    "    X_valid = fout.create_dataset('X_valid', data=valid[0], dtype='int8', compression=\"gzip\")\n",
    "    Y_valid = fout.create_dataset('Y_valid', data=valid[1], dtype='int8', compression=\"gzip\")\n",
    "    X_test = fout.create_dataset('X_test', data=test[0], dtype='int8', compression=\"gzip\")\n",
    "    Y_test = fout.create_dataset('Y_test', data=test[1], dtype='int8', compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Not a dataset (not a dataset)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-98-d2ba684acd0f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\_hl\\dataset.py\u001b[0m in \u001b[0;36mshape\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[1;34m\"\"\"Numpy-style shape tuple giving dataset dimensions\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mwith_phil\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\h5d.pyx\u001b[0m in \u001b[0;36mh5py.h5d.DatasetID.shape.__get__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5d.pyx\u001b[0m in \u001b[0;36mh5py.h5d.DatasetID.shape.__get__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5d.pyx\u001b[0m in \u001b[0;36mh5py.h5d.DatasetID.get_space\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Not a dataset (not a dataset)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-102-79975ee04f47>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dataset_name'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;31m# `data` is now an ndarra\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'value'"
     ]
    }
   ],
   "source": [
    "\n",
    "data = hf.get('dataset_name').value # `data` is now an ndarra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-103-db30ed1691b6>:2: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  a = hf.get('X_train').value # `data` is now an ndarra\n"
     ]
    }
   ],
   "source": [
    "hf = h5py.File('./final_data/invivo_dataset.h5', 'r')\n",
    "a = hf.get('X_train').value # `data` is now an ndarra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.10.0'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h5py.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate  sequences from one hot encoded vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSeqFromOneHotArray(arr):\n",
    "    s = \"\"\n",
    "    for i in range (784):\n",
    "        temp = arr[:,i]\n",
    "        n=np.argmax(temp)\n",
    "        if n==0:\n",
    "            s+=\"A\"\n",
    "        elif n== 1:\n",
    "            s+=\"G\"\n",
    "        elif n== 2:\n",
    "            s+=\"C\"\n",
    "        elif n== 3:\n",
    "            s+=\"T\"\n",
    "    return s\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(269994, 4, 1000, 1)\n",
      "(4, 1000, 1)\n",
      "(4, 1000, 1)\n"
     ]
    }
   ],
   "source": [
    "one_hot_seqs = train[0]\n",
    "print(one_hot_seqs.shape)\n",
    "print(one_hot_seqs[0].shape)\n",
    "a = one_hot_seqs[0]\n",
    "print(a.shape)\n",
    "sequences = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(269994, 4, 1000, 1)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_seqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "269994"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_seqs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 269994/269994 [12:55<00:00, 348.26it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(one_hot_seqs.shape[0])):\n",
    "    sequences.append(getSeqFromOneHotArray(one_hot_seqs[i,:,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences =  269994\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Sequences = \", len(sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Sequences to text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('invivo_sequences.txt', 'wb') as fp:\n",
    "    pickle.dump(sequences, fp)\n",
    "\n",
    "\n",
    "with open ('invivo_sequences.txt', 'rb') as fp:\n",
    "    itemlist = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#another method to do it\n",
    "with open('listfile.txt', 'w') as filehandle:\n",
    "    for listitem in places:\n",
    "        filehandle.write('%s\\n' % listitem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "269994"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(itemlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing labels of training data to the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import save\n",
    "\n",
    "save('label_data.npy', train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int8)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
